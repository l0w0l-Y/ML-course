# -*- coding: utf-8 -*-
"""11-010_Кулакова_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sBXigVNZP8bqAN5QxyFQsdJ5wz34uPtO
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

from sklearn.model_selection import train_test_split # разбиение данных на тренировочные и тестовые

from sklearn.compose import ColumnTransformer # преобразование столбцов
from sklearn.preprocessing import OneHotEncoder # кодирование категориальных переменных
from sklearn.preprocessing import LabelEncoder

from sklearn.preprocessing import StandardScaler # нормализация и масштабирование данных

from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2, f_classif

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier
from sklearn.ensemble import StackingClassifier, BaggingClassifier
from xgboost import XGBClassifier
import lightgbm as lgb

from sklearn.model_selection import cross_val_score # кроссвалидация
from sklearn.model_selection import GridSearchCV # подбор гиперпараметров с кроссвалидацией


from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay

import gc

sns.set()
# %matplotlib inline

import sys
# np.set_printoptions(suppress=True)
# np.set_printoptions(threshold=sys.maxsize)
# np.set_printoptions(precision=3)

DISPLAY_MAX_ROWS = 20 #20
pd.set_option('display.max_rows', DISPLAY_MAX_ROWS)
pd.set_option('display.max_column', 100) # None)
plt.style.use('seaborn-whitegrid')


# plt.rcParams["figure.figsize"] = (20, 15)

import warnings
warnings.filterwarnings('ignore')

"""# Полезные ссылки
* [1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking]()
* [1.11. Ансамблевые методы](https://scikit-learn.ru/1-11-ensemble-methods/)
* [Методы сбора ансамблей алгоритмов машинного обучения: стекинг, бэггинг, бустинг](https://habr.com/ru/articles/561732/)
* [Ансамблевые методы машинного обучения](https://habr.com/ru/articles/571296/)
* [XGBoost Documentation](https://xgboost.readthedocs.io/en/stable/index.html)
* [Catboost Documentation]()
* [Открытый курс машинного обучения. Тема 10. Градиентный бустинг](https://habr.com/ru/companies/ods/articles/327250/)
* [Виды ансамблей](https://neerc.ifmo.ru/wiki/index.php?title=%D0%92%D0%B8%D0%B4%D1%8B_%D0%B0%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B5%D0%B9)
* [Бустинг, AdaBoost - ИТМО](https://neerc.ifmo.ru/wiki/index.php?title=%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3,_AdaBoost)
* [XGBoost - ИТМО](https://neerc.ifmo.ru/wiki/index.php?title=XGBoost)
* [CatBoost - ИТМО](https://neerc.ifmo.ru/wiki/index.php?title=CatBoost&mobileaction=toggle_view_desktop)
* [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)
* [2.3. Решающие деревья - яндекс хэндбук](https://academy.yandex.ru/handbook/ml/article/reshayushchiye-derevya)
* [2.4. Ансамбли в машинном обучении - яндекс хэндбук](https://academy.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii)
* [2.5. Градиентный бустинг - яндекс хэндбук](https://academy.yandex.ru/handbook/ml/article/gradientnyj-busting)
* [Презентация MatrixNet: есть про бустинг](https://cachev2-m9-11.cdn.yandex.net/download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf?lid=226)
* [The Strength of Weak Learnability](http://rob.schapire.net/papers/strengthofweak.pdf)
* [Ensemble methods: bagging, boosting and stacking](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)
* [CatBoost, XGBoost и выразительная способность решающих деревьев](https://habr.com/ru/companies/ods/articles/645887/)
* [XGBoost, LightGBM или CatBoost - какой алгоритм бустинга следует использовать?](https://vk.com/@coeusds-xgboost-lightgbm-ili-catboost-kakoi-algoritm-bustinga-sled)
* [1.13. Feature selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)
* [1.13. Выбор признаков](https://scikit-learn.ru/1-13-feature-selection/)
* [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)

# Загрузка и предобработка данных
"""

dataset = pd.read_csv('/Users/amirmukhamedzhan/Yandex.Disk.localized/Документы/KFU/ML/ML_FALL_2023/data/Data.csv')
dataset

dataset.info()

dataset.Class.value_counts()

d = {2: 0, 4: 1}
dataset["Class"] = dataset["Class"].map(d)
dataset.head()

dataset.Class.value_counts()

X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

X.shape, y.shape

# разбиение  на тренировочные и тестовые данные
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

"""# Что такое ансамбль?

Метод машинного обучения, где несколько моделей (**слабые ученики**) обучаются для решения одной и той же проблемы и объединяются для получения лучших результатов называется ансамблевым методом. Основная предпосылка заключается в том, что результат работы нескольких моделей будет более точен, чем результат только одной модели.

Объединение слабых учеников для улучшения качества модели, уменьшения смещения или разброса, называется сильным учеником.

# Виды ансамблевых методов

* **Стекинг**. Могут рассматриваться разнородные отдельно взятые модели. Существует мета-модель, которой на вход подаются базовые модели, а выходом является итоговый прогноз.

* **Бэггинг**. Рассматриваются однородные модели, которые обучаются независимо и параллельно, а затем их результаты просто усредняются. Ярким представителем данного метода является случайный лес.

* **Бустинг**. При использовании данного метода несколько однородных моделей последовательно обучаются, исправляя ошибки друг друга.

![Снимок экрана 2023-10-19 в 20.12.57.png](attachment:df911e6c-970e-424a-82f7-8829a362b6cb.png)

# StackingClassifier

Стекинг наименее популярний вариант ансамбля.

Основные черты стекинг:
* Может объединить в себе алгоритмы разной природы в качестве базовых. Например, взять метод опорных векторов (SVM), k-ближайших соседей (KNN) в качестве базовых и на основе их результатов обучить логистическую регрессию для классификации.
* Непредсказуемость работы метамодели. Если в случае бэггинга и бустинга существует достаточно четкий и конкретный ансамблевый алгоритм (увидим далее), то здесь метамодель может с течением времени по-разному обучаться на входных данных.

![Снимок экрана 2023-10-19 в 20.15.36.png](attachment:e1ff0028-6423-4b71-93e0-3323ba8f54a6.png)

**Алгоритм обучения выглядит следующим образом**
1. Делим выборку на k фолдов (тот же смысл, что и в кросс-валидации).

2. Для объекта из выборки, который находится в k-ом фолде, делается предсказание слабыми алгоритмами, которые были обучены на k-1 фолдах. Этот процесс итеративен и происходит для каждого фолда.

3. Создается набор прогнозов слабых алгоритмов для каждого объекта выборки.

4. На сформированных низкоуровневыми алгоритмами прогнозах в итоге обучается метамодель.
"""

estimators = [('lr', LogisticRegression()), ('dt', DecisionTreeClassifier())]
classifier = StackingClassifier(estimators=estimators, final_estimator=SVC())
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

"""# Bagging

Наиболее популярный  по сравнению со стекингом. Типичный представитель - Random Forest.

При данном методе базовые алгоритмы являются представителями одного и того же семейства, они обучаются параллельно и почти независимо друг от друга, а финальные результаты лишь агрегируются. Нам необходимо, чтобы на вход слабым алгоритмам подавались разные данные, а не один и тот же набор, ведь тогда результат базовых моделей будет идентичен и смысла в них не будет.

Для того, чтобы понять, каким образом исходный датасет делится для формирования входных выборок для слабых алгоритмов, используется понятие бутстрэпа. При использовании бутстрэпа из исходной выборки берется один случайный элемент, записывается в обучающую выборку, затем возвращается обратно. Так делается n раз, где n - желаемый размер обучающей выборки. Существует правило, что в обучающей выборке в итоге будет `~ 0.632*n` разных объектов. Таким образом, должны сформироваться m обучающих выборок для m слабых алгоритмов.

Бутстрэп выборки являются в значительной степени независимыми. Отчасти поэтому и говорят, что базовые алгоритмы обучаются на выборках независимо.

Что касается агрегации выходов базовых алгоритмов, то в случае задачи классификации зачастую просто выбирается наиболее часто встречающийся класс, а в случае задачи регрессии выходы алгоритмов усредняются (см рис.). В формуле под ai подразумеваются выходы базовых алгоритмов.
![Снимок экрана 2023-10-19 в 20.57.22.png](attachment:c848e6ab-9488-4acb-a30b-ff26c4482adf.png)

![Снимок экрана 2023-10-19 в 20.57.52.png](attachment:2a2dc201-c7db-4848-bf39-aa4db4dceb02.png)

Бэггинг направлен на уменьшение разброса (дисперсии) в данных, и зачастую данный прием предстает в виде алгоритма случайного леса, где слабые модели - это довольно глубокие случайные деревья. Однако, при построении случайного леса используется еще один прием, такой как метод случайных подпространств. Мало того, что благодаря бутсрэпу выбираются некоторые объекты нашего датасета, так еще и выбирается случайное подмножество признаков. В итоге, наша условная матрица признаков уменьшается как по строкам, так и столбцам (см. рис. ниже). Это помогает действительно снизить корреляцию между слабыми учениками.

![Снимок экрана 2023-10-19 в 20.59.23.png](attachment:8f079a70-c072-4d92-a47c-f36130f6d45c.png)

# RandomForest
"""

classifier = RandomForestClassifier()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

accuracies

"""# BaggingClassifier"""

classifier = BaggingClassifier(estimator=LogisticRegression())
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

accuracies

"""# Бустинг

В данном случае, модели уже не обучаются отдельно друг от друга, а каждая следующая правит ошибки предыдущей. То есть можно сказать, что если один слабый алгоритм не смог выявить какую-либо закономерность в данных, так как это было для него сложно, то следующая модель должна сделать это. Но из данного подхода вытекает недостаток: работу алгоритма трудно распараллелить из-за зависимости предыдущего и последующего шагов.

Бустинг направлен скорее на уменьшение смещения в данных, чем на снижение разброса в них. Поэтому в качестве базовых алгоритмов могут браться модели с достаточно высоким смещением, например, неглубокие случайные деревья.

Типичными представителями бустинга являются две модели: градиентный бустинг и AdaBoost. Обе по-разному решают одну и ту же оптимизационную задачу по поиску итоговой модели, представляющей собой взвешенную сумму слабых алгоритмов.

![Снимок экрана 2023-10-19 в 21.03.04.png](attachment:ac96c868-3462-42df-a536-3dfceee9cd1e.png)

Градиентный бустинг использует типичный алгоритм градиентного спуска для решения задачи. Когда приходит время добавить новый слабый алгоритм в ансамбль делается следующее:

1. Находится оптимальный вектор сдвига, улучшающий предыдущий ансамбль алгоритмов.

2. Этот вектор сдвига является антиградиентом от функции ошибок работы предыдущего ансамбля моделей

3. Благодаря вектору сдвигов мы знаем, какие значения должны принимать объекты обучающей выборки

4. А поскольку нам надо найти очередной алгоритм в композиции, то находим тот, при использовании которого минимизируется отклонение ответов от истинных

Градиентный бустинг - это в своем роде обобщение AdaBoost, поэтому, возможно, зачастую его и изучают первым.

# AdaBoostClassifier

Данный алгоритм сначала обучает первую базовую модель(допустим деревья решений) на тренировочном наборе. Относительный вес некорректно предсказанных значений увеличивается. На вход второй базовой модели подаются обновлённые веса и модель обучается, после чего вырабатываются прогнозы и цикл повторяется.

Результат работы AdaBoost - это средневзвешенная сумма каждой модели. Спрогнозированным значением ансамбля будет тот, который получает большинство взвешенных голосов

![image.png](attachment:ce0cfd48-03e2-4ba4-9201-89c79ee33be0.png)

C - результат работы ансамбля, W - вес,
X - значение прогнозатора

Adaboost обновляет веса объектов на каждой итерации. Веса хорошо классифицированных объектов уменьшаются относительно весов неправильно классифицированных объектов. Модели, которые работают лучше, имеют больший вес в окончательной модели ансамбля.

При адаптивном бустинге используется итеративный метод (добавляем слабых учеников одного за другим, просматривая каждую итерацию, чтобы найти наилучшую возможную пару (коэффициент, слабый ученик) для добавления к текущей модели ансамбля) изменения весов. Он работает быстрее, чем аналитический метод.
"""

classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=100, random_state=12)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

accuracies

"""# GradientBoostingClassifier

Градиентный бустинг обучает слабые модели последовательно, исправляя ошибки предыдущих. Результатом градиентного бустинга также является средневзвешенная сумма результатов моделей. Принципиальное отличие от Adaboost это способ изменения весов. Адаптивный бустинг использует итеративный метод оптимизации. Градиентный бустинг оптимизируется с помощью градиентного спуска.

Таким образом градиентный бустинг - обобщение адаптивного бустинга для дифференцируемых функций.
"""

classifier = GradientBoostingClassifier(max_depth=2, n_estimators=150,
                                      random_state=12, learning_rate=1)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

accuracies

"""# XGBoost, LightGBM и CatBoost

[XGBoost, LightGBM или CatBoost - какой алгоритм бустинга следует использовать?](https://vk.com/@coeusds-xgboost-lightgbm-ili-catboost-kakoi-algoritm-bustinga-sled)

Существует несколько различных вариаций GBDT – это XGBoost, LightGBM и CatBoost.

Они отличаются друг от друга по нескольким критериям – симметричность деревьев, метод разбиения объектов, обработка категориальных признаков, интерпретация пропущенных значений, обработка текстовых признаков.

* В CatBoost деревья симметричные на каждом уровне, в двух других алгоритмах – ассиметричные. Таким образом, в LightGBM алгоритме деревья растут по листьям, горизонтально (leaf-wise growth), а в XGBoost деревья растут по уровням, вертикально (level-wise growth).

* В CatBoost используется жадный алгоритм разбиения наблюдений, в LightGBM используется Gradient-based One-Side Sampling, основанный на значениях градиента для наблюдений, а в XGBoost разбиение работает по предварительной сортировке значений признаков.

* На вход CatBoost могут подаваться категориальные признаки, LightGBM может принимать их на вход только в числовом формате, однако можно в порядковом виде. А XGBoost не может работать с порядковыми данными, категориальные признаки должны подаваться только после кодирования.

# XGBoost

* `pip install xgboost`
* `conda install -c conda-forge xgboost`
"""

classifier = XGBClassifier()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

accuracies

"""# CatBoost

* `pip install catboost`
* `conda install -c conda-forge catboost`
"""

gc.collect()

classifier = CatBoostClassifier(verbose=0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

"""# LightGBM

* `pip install lightgbm`
* `conda install -c conda-forge lightgbm`
"""

classifier = lgb.LGBMClassifier()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

accuracies

"""# ДЗ:

1. Использовать датасет `telecom_churn.csv`
2. Обучить LogisticRegression. Использовать в качестве `baseline`.
3. Обучить на этом датасете все три вида ансамблей, использованных в этом ноутбуке. Сравнить с baseline
4. Использовать PCA для понижения размерности, или использовать статистическим методом отбор признаков SelectKBest. Затем обучить LogisticRegression в качестве `baseline`, и обучить все три вида ансамблей, рассмотренных в этом ноутбуке.
5. Сравнить результаты между собой до понижения размерности или отбора признаков, так и после них.
"""

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/My Drive/ML/telecom_churn.csv'
df = pd.read_csv(path)
df

df.info()

df.fillna(df.mode(), inplace=True)
df = pd.get_dummies(df, columns=['State', 'International plan', 'Voice mail plan', 'Churn'], drop_first= True )
df

X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)

regression = LogisticRegression()
regression.fit(X_train, y_train)
print(regression.score(X_test, y_test))
accuracies = cross_val_score(estimator = regression, X = X_train, y = y_train, cv = 10)
print(accuracies.mean()*100)

estimators = [('lr', LogisticRegression()), ('dt', DecisionTreeClassifier())]
classifier = StackingClassifier(estimators=estimators, final_estimator=SVC())
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print(accuracies.mean()*100)

classifier = BaggingClassifier(estimator=LogisticRegression())
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print(accuracies.mean()*100)

classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=100, random_state=12)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print(accuracies.mean()*100)

classifier = GradientBoostingClassifier(max_depth=2, n_estimators=150,
                                      random_state=12, learning_rate=1)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print(accuracies.mean()*100)

"""Итоги:
*   LogisticRegression - 0.8650674662668666
*   StackingClassifier - 91.25960969840332
*   BaggingClassifier - 85.55870012108923
*   AdaBoostClassifier - 92.79744304581679
*   GradientBoostingClassifier - 91.10909295711188


"""

pca = PCA().fit(X)
X_pca = pca.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state = 0)

regression = LogisticRegression()
regression.fit(X_train, y_train)
print(regression.score(X_test, y_test))
accuracies = cross_val_score(estimator = regression, X = X_train, y = y_train, cv = 10)
print(accuracies.mean()*100)

estimators = [('lr', LogisticRegression()), ('dt', DecisionTreeClassifier())]
classifier = StackingClassifier(estimators=estimators, final_estimator=SVC())
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print(accuracies.mean()*100)

classifier = BaggingClassifier(estimator=LogisticRegression())
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print(accuracies.mean()*100)

classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=100, random_state=12)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print(accuracies.mean()*100)

classifier = GradientBoostingClassifier(max_depth=2, n_estimators=150,
                                      random_state=12, learning_rate=1)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=classifier.classes_)
disp.plot()
plt.show()

accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print(accuracies.mean()*100)

"""Итоги:
*   LogisticRegression - 86.00884233054546 и 86.00884233054546
*   StackingClassifier - 91.25960969840332 и 91.56022077666076
*   BaggingClassifier - 85.55870012108923 и 85.97026273549041
*   AdaBoostClassifier - 92.79744304581679 и 90.99701500943368
*   GradientBoostingClassifier - 91.10909295711188 и 90.0977162006139
"""