# -*- coding: utf-8 -*-
"""11-010_Кулакова_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19YCsQUIUTwYjvNk3KSKDjcO69Hfy8zEG

# Supervised Learning - Обучение с учителем

# Введение Регрессия

Задача регрессии (предсказания) относится к типовым задачам машинного обучения с учителем. Задача заключается в том, чтобы научиться предсказывать значения целевой (объясняемой) переменной на основе знаний о значениях этой переменной при заданных значениях признаков (объясняющих переменных). Данная задача весьма часто встречается на практике.

# Полезные ссылки
* [Линейные модели](https://scikit-learn.ru/1-1-linear-models/)
* [Метрики регрессии](https://scikit-learn.ru/3-3-metrics-and-scoring-quantifying-the-quality-of-predictions/#regression-metrics)
* [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
* [Открытый курс машинного обучения. Тема 3. Классификация, деревья решений и метод ближайших соседей](https://habr.com/ru/companies/ods/articles/322534/)
* [Открытый курс машинного обучения. Тема 4. Линейные модели классификации и регрессии](https://habr.com/ru/companies/ods/articles/323890/#1-lineynaya-regressiya)
* [Умная нормализация данных: категориальные и порядковые данные, “парные” признаки](https://habr.com/ru/articles/527860/)
* [Умная нормализация данных](https://habr.com/ru/articles/527334/)
* [Категориальные признаки](https://habr.com/ru/articles/666234/)
* [Преобразования данных](https://scikit-learn.ru/category/dataset_transformations/)
* [Decision Tree](https://scikit-learn.ru/1-10-decision-trees/)

# Импорт модулей
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split # разбиение данных на тренировочные и тестовы

from sklearn.compose import ColumnTransformer # преобразование столбцов
from sklearn.preprocessing import OneHotEncoder # кодирование категориальных переменных

from sklearn.preprocessing import StandardScaler, MinMaxScaler # нормализация и масштабирование данных

from sklearn.linear_model import LinearRegression # Простая линейная регрессия
from sklearn.preprocessing import PolynomialFeatures # для полиномиальной регрессии
from sklearn.svm import SVR # метод опорных векторов
from sklearn.tree import DecisionTreeRegressor # Дерево решений
from sklearn.ensemble import RandomForestRegressor # Лес деревье решений

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error

import gc

sns.set()
# %matplotlib inline

import sys
np.set_printoptions(suppress=True)
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(precision=3)

DISPLAY_MAX_ROWS = 100 #20
pd.set_option('display.max_rows', DISPLAY_MAX_ROWS)
pd.set_option('display.max_column', 100) # None)
plt.style.use('seaborn-whitegrid')


# plt.rcParams["figure.figsize"] = (20, 15)

import warnings
warnings.filterwarnings('ignore')

"""# Простая линейная регрессия

## Немного теории

Линейная регрессия (Linear regression) — модель зависимости целевой (объясняемой) переменной от одной или нескольких других независимых (объясняющих) переменных (которые называются факторами или регрессорами) с использованием линейной функции $\vec{y}=A\vec{x}+\vec{B}$, где $\vec{y}$ - вектор целевых значений, $A$ - матрица весов для имеющихся значений признаков, $\vec{B}$ - вектор значений смещений, $\vec{x}$ - имеющиеся значения признаков.

Матрица $A$ имеет размерность $m$ на $n$, где $m$ - количество признаков, $n$ - количество наблюдений признаков.

Например, зависимость

$$
ВЕС = A \cdot РОСТ + B
$$

линейно связывает рост и вес человека, позволяя по значению веса предположить значение роста, для этого определяется константа (коэффициент, вес) $a$ и смещение $b$.

Пример более сложной зависимости:

$$
ЦЕНА = A \cdot КАЧЕСТВО + B \cdot ВРЕМЯ + C \cdot ЗАТРАТЫ + D
$$


Также более сложные зависимости можно строить с помощью конструирования новых признаков с использованием существующих (рассматривая, например, степени известных признаков):

$$
y(x)=a x^2 + b x + c,
$$

в которой объясняющими переменными (признаками)  являются $x$ и $x^2$, а искомых коэффициентов три: $a$, $b$ и $c$.

В примере ниже показано приближение множества точек моделью $y(x) = Ax + B$ (линейная зависимость, один признак $x$) и моделью $y(x)=Ax^2 + Bx + C$ (квадратичная зависимость, два признака $x^2$ и $x$).
"""

xs = np.linspace(0, 5, 20)
ys = xs**2 + np.random.randn(20)
plt.scatter(xs, ys, c='b')
plt.plot(xs, 5*xs, c='r', label='Приближение $y=Ax+B$')
plt.plot(xs, xs**2+xs, c='g', label='Приближение $y=Ax^2+Bx+c$')
plt.grid(True)
plt.legend()
plt.show()

"""В общем случае, если задан вектор признаков $\vec{x}=(x_1, \cdots, x_n)$ и вектор весов $\vec{w}=(w_0, w_1, \cdots, w_n)$, то их линейной комбинацией является выражение

$$
y = w_0 + x_1 w_1 + \cdots + w_n x_n,
$$

или, при добавлении единицы в вектор признаков $\vec{x}=(1, x_1, \cdots, x_n)$ в более коротком виде
$$
y=w^T x.
$$

В самом простом случае (одна объясняющая переменная, один признак) модель линейной регрессии представляет собой линейную функцию $y(x)=Ax+b$. Графически это означает, что мы пытаемся построить прямую линию $y(x) = Ax+B$, которая проходит "как можно ближе" к таблично заданным точкам $(x_1,y_1), \cdots,(x_n,y_n)$.
"""

xs = np.linspace(0, 5, 10)
ys = xs**2 + np.random.randn(10)
plt.scatter(xs, ys, c='b')
plt.plot(xs, 5*xs-2, c='r')
for x, y in zip(xs, ys):
    plt.plot([x,x], [5*x-2,y], c='g')
plt.grid(True)
plt.show()

"""Параметр, характеризующий ошибку модели, очевидно связан с длинной зеленых отрезков на рисунке (т.е. расстояний между известными заданными точками $y_i$ и значениями, предсказанными моделью $Ax_i+B$).

Поэтому в качестве меры ошибки берется сумма среднеквадратичных отклонений, для наиболее оптимальной модели эта мера должна быть минимальна $$F(A,B)=\sqrt{\sum_{i=1}^{n}{\left[ Ax_i + B -y_i\right]^2}}\to \min$$

Решение этой задачи оптимизации можно получить в виде точной формулы. Для этого используется тот факт, что экстремум (минимум) достигается в точке, в которой частные производные целевой функции $F(A,B)$ равны нулю:
$$
\frac{\partial F(A,B)}{\partial A}=0,~\frac{\partial F(A,B)}{\partial B}=0.
$$
Из этих двух уравнений получается система линейных уравнений для нахождения коэффициентов $A$ и $B$:
$$
A \cdot n + B \cdot \sum_{i=1}^{n}{x_i}=\sum_{i=1}^{n}{y_i}
$$
$$
A \sum_{i=1}^{n}{x_i} + B \cdot \sum_{i=1}^{n}{x_i ^2}=\sum_{i=1}^{n}{x_i y_i}.
$$

Из этой системы в явном виде находятся коэффициенты регрессии $A$ и $B$, что и дает решение задачи. Однако у такого подхода есть существенный минус - при решении СЛАУ методом Крамера или методом обратной матрицы необходимо вычислять определитель, что неэффективно для очень больших матриц (когда много признаков) или для матриц, определитель которых близок к нулю.

Поэтому в большинстве практических задач более эффективными являются приближенные численные методы, которые позволяют найти решение с заданной точностью с меньшими ресурсами (например, градиентный метод).

# Simple Linear Regression - sklearn
"""

# загрузка данных
from google.colab import drive

path = '/content/drive/My Drive/Salary_Data.csv'
drive.mount('/content/drive')
dataset = pd.read_csv(path)

X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

dataset

print(X.shape)
print(X)

print(y.shape)
print(y)

gc.collect()

# разбиение данных на тренировочные и тестовые данные
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state=0, shuffle=True)

# обучение модели
regressor = LinearRegression()
regressor.fit(X_train, y_train)

regressor.coef_, regressor.intercept_ # коэффициенты модели

"""$$y=9345.94244312*x+26816.192244031183$$"""

y_pred = regressor.predict(X_test) # предсказание модели

y_pred, y_test

regressor.score(X_train, y_train), regressor.score(X_test, y_test) # оченка точности модели. По умолчанию R_2

"""**Метрики:**

**1. Средняя абсолютная ошибка**

Функция `mean_absolute_error` вычисляет среднюю абсолютную погрешность , риск метрики , соответствующей ожидаемого значение абсолютной потери или ошибок $l1$-нормальная потеря.

Если $\hat{y}_i$ прогнозируемое значение $i$-й образец, и $y_i$ — соответствующее истинное значение, тогда средняя абсолютная ошибка (MAE), оцененная за
 определяется как
$$\text{MAE}(y, \hat{y}) = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}}-1} \left| y_i — \hat{y}_i \right|.$$

**2. Среднеквадратичная ошибка**

Функция `mean_squared_error` вычисляет среднюю квадратическую ошибку , риск метрики , соответствующую ожидаемое значение квадрата (квадратичной) ошибки или потерю.

Если $\hat{y}_i$ прогнозируемое значение $i$-й образец, и $y_i$ — соответствующее истинное значение, тогда среднеквадратичная ошибка (MSE), оцененная на $n_{samples}$ определяется как

 $$\text{MSE}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples} — 1} (y_i — \hat{y}_i)^2.$$

**3. Медианная абсолютная ошибка**

Это `median_absolute_error` особенно интересно, потому что оно устойчиво к выбросам. Убыток рассчитывается путем взятия медианы всех абсолютных различий между целью и прогнозом.

Если $\hat{y}_i$ прогнозируемое значение $i$-й образец и  $y_i$ — соответствующее истинное значение, тогда средняя абсолютная ошибка (MedAE), оцененная на $n_{samples}$ определяется как

$$\text{MedAE}(y, \hat{y}) = \text{median}(\mid y_1 — \hat{y}_1 \mid, \ldots, \mid y_n — \hat{y}_n \mid).$$

**4. R² score, коэффициент детерминации**

Функция `r2_score` вычисляет коэффициент детерминации , как правило , обозначенный как R².

Он представляет собой долю дисперсии (y), которая была объяснена независимыми переменными в модели. Он обеспечивает показатель степени соответствия и, следовательно, меру того, насколько хорошо невидимые выборки могут быть предсказаны моделью через долю объясненной дисперсии.

Поскольку такая дисперсия зависит от набора данных, R² не может быть значимо сопоставимым для разных наборов данных. Наилучшая возможная оценка — 1,0, и она может быть отрицательной (потому что модель может быть произвольно хуже). Постоянная модель, которая всегда предсказывает ожидаемое значение y, игнорируя входные характеристики, получит оценку R² 0,0.

Если $\hat{y}_i$ прогнозируемое значение $i$-й образец и  соответствующее истинное значение для общего $n$ образцов, расчетный R² определяется как:

$$R^2(y, \hat{y}) = 1 — \frac{\sum_{i=1}^{n} (y_i — \hat{y}i)^2}{\sum{i=1}^{n} (y_i — \bar{y})^2}$$

где $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$ и $\sum_{i=1}^{n} (y_i — \hat{y}i)^2 = \sum{i=1}^{n} \epsilon_i^2$

Обратите внимание, что `r2_score` вычисляется нескорректированное R² без поправки на смещение выборочной дисперсии y.
"""

print(f'Средняя абсолютная ошибка: {mean_absolute_error(y_test, y_pred)}')
print(f'средняя квадратичная ошибка: {mean_squared_error(y_test, y_pred)}')
print(f'Коэффициент детерминации: {r2_score(y_test, y_pred)}')
print(f'Медианная абсолютная ошибка: {median_absolute_error(y_test, y_pred)}')

# Визуализация на тренировчных данных
plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Training set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()

# визуализация на тестовых данных
plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Test set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()

"""# Допущения линейной регрессии

![image.png](attachment:d83b4598-aa49-476c-9bbe-68e1ad60e79f.png)

# ДЗ №1

1. Построить функцию линейной регрессии. Найти значение при $\alpha=20000$ с точностью двух знаков после запятой.
$$
\begin{array}{c|c|c|c|c|c|c|c}
t & 0 & 100 & 150 & 200 & 250 & 300 & 360
\\
\hline
\alpha & 18179 & 18216 & 18261 & 18323 & 18403 & 18500 & 18641
\end{array}
$$

2. Построить функцию линейной регрессии. Найти значение при $S=100$ с точностью двух знаков после запятой.
$$
\begin{array}{c|c|c|c|c|c|c|c|c}
\theta & 273 & 283 & 288 & 294 & 313 & 333 & 353 & 373
\\
\hline
S & 29,4 & 33,3 & 35,2 & 37,2 & 45,8 & 55,2 & 65,6 & 77,3
\end{array}
$$

3. Построить функцию линейной регрессии. Найти значение при $P=20$ с точностью двух знаков после запятой.
$$
\begin{array}{c|c|c|c|c|c|c|c}
\nu & 3,33 & 1,63 & 0,87 & 0,43 & 0,27 & 0,17 & 0,12
\\
\hline
P & 0,48 & 1,04 & 2,03 & 4,25 & 7,16 & 11,49 & 17,59
\end{array}
$$

4. Построить функцию линейной регрессии. Найти значение при $S=1$ с точностью двух знаков после запятой.
$$
\begin{array}{c|c|c|c|c|c|c|c}
V & 2,40 & 3,50 & 5,20 & 6,89 & 10,00 & 12,55 & 13,67
\\
\hline
S & 0,014 & 0,028 & 0,056 & 0,119 & 0,226 & 0,315 & 0,566
\end{array}
$$


**Визуализировать результаты на тестовых и тренировочных данных, посчитать метрики, вывести коэффициенты моделей**
"""

array = [[18179, 0], [18216, 100], [18261, 150], [18323, 200], [18403, 250], [18500, 300], [18641, 360]]
dataset = pd.DataFrame(array, columns=['t', 'a'])
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
# разбиение данных на тренировочные и тестовые данные
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state=0, shuffle=True)
# обучение модели
regressor = LinearRegression()
regressor.fit(X_train, y_train)
print(regressor.coef_, regressor.intercept_) # коэффициенты модели
y_pred = regressor.predict(X_test).round(2) # предсказание модели
print(y_pred, y_test)
alpha_pred = regressor.predict([[20000]]).round(2)
print(alpha_pred)
print(regressor.score(X_train, y_train), regressor.score(X_test, y_test)) # оценка точности модели. По умолчанию R_2
print(f'Средняя абсолютная ошибка: {mean_absolute_error(y_test, y_pred)}')
print(f'средняя квадратичная ошибка: {mean_squared_error(y_test, y_pred)}')
print(f'Коэффициент детерминации: {r2_score(y_test, y_pred)}')
print(f'Медианная абсолютная ошибка: {median_absolute_error(y_test, y_pred)}')
# Визуализация на тренировчных данных
plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Тренировочные данные')
plt.xlabel('a')
plt.ylabel('t')
plt.show()
# визуализация на тестовых данных
plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Тестовые данные')
plt.xlabel('a')
plt.ylabel('t')
plt.show()

array = [[273, 29.4], [283, 33.3], [288, 35.2], [294, 37.2], [314, 45.8], [333, 55.2], [353, 65.6], [373, 77.3]]
dataset = pd.DataFrame(array, columns=['S', 'teta'])
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
# разбиение данных на тренировочные и тестовые данные
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state=0, shuffle=True)
# обучение модели
regressor = LinearRegression()
regressor.fit(X_train, y_train)
print(regressor.coef_, regressor.intercept_) # коэффициенты модели
y_pred = regressor.predict(X_test).round(2) # предсказание модели
print(y_pred, y_test)
alpha_pred = regressor.predict([[100]]).round(2)
print(alpha_pred)
print(regressor.score(X_train, y_train), regressor.score(X_test, y_test)) # оценка точности модели. По умолчанию R_2
print(f'Средняя абсолютная ошибка: {mean_absolute_error(y_test, y_pred)}')
print(f'средняя квадратичная ошибка: {mean_squared_error(y_test, y_pred)}')
print(f'Коэффициент детерминации: {r2_score(y_test, y_pred)}')
print(f'Медианная абсолютная ошибка: {median_absolute_error(y_test, y_pred)}')
# Визуализация на тренировчных данных
plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Тренировочные данные')
plt.xlabel('S')
plt.ylabel('teta')
plt.show()
# визуализация на тестовых данных
plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Тестовые данные')
plt.xlabel('S')
plt.ylabel('teta')
plt.show()

array = [[3.33, 0.48], [1.63, 1.04], [0.87, 2.03], [0.43, 4.25],[0.27, 7.16], [0.17, 11.49], [0.12, 17.59] ]
dataset = pd.DataFrame(array, columns=['v', 'p'])
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
# разбиение данных на тренировочные и тестовые данные
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state=0, shuffle=True)
# обучение модели
regressor = LinearRegression()
regressor.fit(X_train, y_train)
print(regressor.coef_, regressor.intercept_) # коэффициенты модели
y_pred = regressor.predict(X_test).round(2) # предсказание модели
print(y_pred, y_test)
alpha_pred = regressor.predict([[20]]).round(2)
print(alpha_pred)
print(regressor.score(X_train, y_train), regressor.score(X_test, y_test)) # оценка точности модели. По умолчанию R_2
print(f'Средняя абсолютная ошибка: {mean_absolute_error(y_test, y_pred)}')
print(f'средняя квадратичная ошибка: {mean_squared_error(y_test, y_pred)}')
print(f'Коэффициент детерминации: {r2_score(y_test, y_pred)}')
print(f'Медианная абсолютная ошибка: {median_absolute_error(y_test, y_pred)}')
# Визуализация на тренировчных данных
plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Тренировочные данные')
plt.xlabel('v')
plt.ylabel('p')
plt.show()
# визуализация на тестовых данных
plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Тестовые данные')
plt.xlabel('v')
plt.ylabel('p')
plt.show()

array = [[2.40, 0.014], [3.50, 0.028], [5.2, 0.056], [6.89, 0.119], [10.0, 0.226], [12.55, 0.315], [13.67, 0.566] ]
dataset = pd.DataFrame(array, columns=['V', 'S'])
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
# разбиение данных на тренировочные и тестовые данные
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state=0, shuffle=True)
# обучение модели
regressor = LinearRegression()
regressor.fit(X_train, y_train)
print(regressor.coef_, regressor.intercept_) # коэффициенты модели
y_pred = regressor.predict(X_test).round(2) # предсказание модели
print(y_pred, y_test)
alpha_pred = regressor.predict([[1]]).round(2)
print(alpha_pred)
print(regressor.score(X_train, y_train), regressor.score(X_test, y_test)) # оценка точности модели. По умолчанию R_2
print(f'Средняя абсолютная ошибка: {mean_absolute_error(y_test, y_pred)}')
print(f'средняя квадратичная ошибка: {mean_squared_error(y_test, y_pred)}')
print(f'Коэффициент детерминации: {r2_score(y_test, y_pred)}')
print(f'Медианная абсолютная ошибка: {median_absolute_error(y_test, y_pred)}')
# Визуализация на тренировчных данных
plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Тренировочные данные')
plt.xlabel('V')
plt.ylabel('S')
plt.show()
# визуализация на тестовых данных
plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Тестовые данные')
plt.xlabel('V')
plt.ylabel('S')
plt.show()

"""# Множественная линейная регрессия

![image.png](attachment:c8c2c055-86bd-4fa4-a90a-589e8c8520dc.png)
"""

path = '/Users/amirmukhamedzhan/Yandex.Disk.localized/Документы/KFU/ML/ML_FALL_2023/data/50_Startups.csv'

dataset = pd.read_csv(path)
dataset

X = dataset.iloc[:, :-1].values # предикторы
y = dataset.iloc[:, -1].values # отклик

X.shape

X

y.shape

y

# Преобразование категориальных переменных
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')
X = np.array(ct.fit_transform(X))

X # после преобразования

# разбиение на тренировочные и тестовые данные
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

regressor = LinearRegression()
regressor.fit(X_train, y_train)

y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

pd.DataFrame(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1), columns=['y_pred', 'y_test'])

regressor.score(X_train, y_train), regressor.score(X_test, y_test)

regressor.coef_, regressor.intercept_

"""$$y=8.66\times 10^{1} * x_1 - 8.73\times 10^{2} * x_2 + 7.86\times 10^{2} * x_3 + 7.73\times 10^{-1} * x_4 + 3.29\times 10^{-2} + x_5 + 3.66\times 10^{-2} * x_6 + 42467.5292485363$$"""

print(f'Средняя абсолютная ошибка: {mean_absolute_error(y_test, y_pred)}')
print(f'средняя квадратичная ошибка: {mean_squared_error(y_test, y_pred)}')
print(f'Коэффициент детерминации: {r2_score(y_test, y_pred)}')
print(f'Медианная абсолютная ошибка: {median_absolute_error(y_test, y_pred)}')

"""# Полиномиальная регрессия

![image.png](attachment:da66b094-0377-496c-b51b-13e95c810f41.png)
"""

path = '/Users/amirmukhamedzhan/Yandex.Disk.localized/Документы/KFU/ML/data/Position_Salaries.csv'

dataset = pd.read_csv(path)

dataset

X = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values

X.shape

X

y.shape

y

lin_reg = LinearRegression() # простая линейная регрессия
lin_reg.fit(X, y)

lin_reg.coef_, lin_reg.intercept_

poly_reg = PolynomialFeatures(degree = 4) # полиномиальная регрессия
X_poly = poly_reg.fit_transform(X)
lin_reg_2 = LinearRegression()
lin_reg_2.fit(X_poly, y)

X_poly

lin_reg_2.coef_, lin_reg_2.intercept_

lin_reg.score(X, y), lin_reg_2.score(X_poly, y) # оценки моделей линейной регрессии и полиномиальной регрессии

# Визуализация линейной регрессииы
plt.scatter(X, y, color = 'red')
plt.plot(X, lin_reg.predict(X), color = 'blue')
plt.title('Truth or Bluff (Linear Regression)')
plt.xlabel('Position Level')
plt.ylabel('Salary')
plt.show()

# Визуализация полиномиальной регрессии
plt.scatter(X, y, color = 'red')
plt.plot(X, lin_reg_2.predict(poly_reg.fit_transform(X)), color = 'blue')
plt.title('Truth or Bluff (Polynomial Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

#  Визуализация результатов полиномиальной регрессии (для более высокого разрешения и сглаживания кривой)
X_grid = np.arange(min(X), max(X), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, lin_reg_2.predict(poly_reg.fit_transform(X_grid)), color = 'blue')
plt.title('Truth or Bluff (Polynomial Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

lin_reg.predict([[6.5]]) # предсказание моделью линейной регрессии

lin_reg_2.predict(poly_reg.fit_transform([[6.5]])) # предсказание моделью полиномиальной регрессией

dataset

"""# Support Vector Machine (SVM) для регрессии

* [Метод опорных векторов (SVM)](https://neerc.ifmo.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2_(SVM))

* [SVM sklearn](https://scikit-learn.ru/1-4-support-vector-machines/)

* [SVR class sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR)

**SVM (Support Vector Machine)**, или метод опорных векторов, - это мощный алгоритм машинного обучения, который используется как для задачи классификации, так и для задачи регрессии. Он был разработан Владимиром Вапником и Алексеем Червоненкисом в 1960-х годах.

SVM основывается на концепции поиска оптимальной гиперплоскости в многомерном пространстве данных, которая наилучшим образом разделяет точки данных разных классов в случае классификации или наилучшим образом аппроксимирует данные в случае регрессии. Гиперплоскость - это линейное разделение, которое максимизирует отступы (margins) между точками данных и самой гиперплоскостью.

Вот ключевые концепции и компоненты SVM:

1. **Гиперплоскость**: Это линейное разделение между двумя классами в задаче классификации или аппроксимация в задаче регрессии. Гиперплоскость выбирается таким образом, чтобы максимизировать отступы между точками данных и самой гиперплоскостью.

2. **Опорные векторы**: Опорные векторы - это точки данных, которые лежат ближе всего к гиперплоскости и играют ключевую роль в определении гиперплоскости. Они используются для определения отступов и, следовательно, влияют на обучение модели.

3. **Ядра**: SVM может использовать различные ядра (например, линейные, полиномиальные, радиально-базисные функциональные (RBF) и другие) для преобразования данных в более высокоразмерное пространство. Ядра позволяют SVM решать задачи, которые не могут быть линейно разделены в исходном пространстве.

4. **Параметр регуляризации C**: Параметр C в SVM контролирует баланс между максимизацией отступов и минимизацией ошибок на обучающих данных. Большие значения C могут привести к более жесткой классификации, что может вызвать переобучение, тогда как маленькие значения C могут привести к более широким отступам, но могут допускать больше ошибок.

5. **Многоклассовая классификация**: SVM по умолчанию предназначен для двоичной классификации, но его можно расширить до задач многоклассовой классификации с использованием методов, таких как "один против всех" (one-vs-all) или "один против одного" (one-vs-one).

SVM является мощным и гибким методом машинного обучения и широко используется в различных областях, включая компьютерное зрение, обработку естественного языка, биоинформатику и многие другие.

![image.png](attachment:a918e062-a897-40e2-b8a5-6c2248611548.png)

**Алгоритм работает в предположении, что чем больше расстояние (зазор) между разделяющей гиперплоскостью и объектами разделяемых классов, тем меньше будет средняя ошибка классификатора.**

**Преимущества и недостатки SVM**

*Преимущества SVM перед методом стохастического градиента и нейронными сетями:*

- Задача выпуклого квадратичного программирования хорошо изучена и имеет единственное решение.
- Метод опорных векторов эквивалентен двухслойной нейронной сети, где число нейронов на скрытом слое определяется автоматически как число опорных векторов.
- Принцип оптимальной разделяющей гиперплоскости приводит к максимизации ширины разделяющей полосы, а следовательно, к более уверенной классификации.

*Недостатки классического SVM:*

- Неустойчивость к шуму: выбросы в исходных данных становятся опорными объектами-нарушителями и напрямую влияют на построение разделяющей гиперплоскости.
- Не описаны общие методы построения ядер и спрямляющих пространств, наиболее подходящих для конкретной задачи.
- Нет отбора признаков.
- Необходимо подбирать константу `C` при помощи кросс-валидации.

**SVR (Support Vector Regression)** - это метод машинного обучения, который используется для задачи регрессии, то есть для прогнозирования непрерывных числовых значений на основе обучающих данных. Он является вариацией метода опорных векторов (SVM), который обычно применяется для задач классификации.

Основная идея SVR заключается в поиске гиперплоскости, которая наилучшим образом соответствует обучающим данным в задаче регрессии. Гиперплоскость максимизирует отступы (margins) между точками данных и самой гиперплоскостью. Важно отметить, что SVR также использует "мягкие" граничные условия, что позволяет некоторым точкам данных находиться за пределами отступов.

![image.png](attachment:255cb688-3b0b-4885-b9c9-6d0fcc83d064.png)

![image.png](attachment:c1374d21-8cf1-40fa-975d-e8550a4392f6.png)
"""

gc.collect()

path = '/Users/amirmukhamedzhan/Yandex.Disk.localized/Документы/KFU/ML/data/Position_Salaries.csv'

dataset = pd.read_csv(path)

dataset

X = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values

print(X)

print(y)

y = y.reshape(len(y),1)
y.shape

y

"""## Масштабирование признаков

https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler

![image.png](attachment:3cdcb37f-c778-4c4b-9659-31cb717e8164.png)
"""

# Масштабирование признаков
sc_X = StandardScaler()
sc_y = StandardScaler()
X = sc_X.fit_transform(X)
y = sc_y.fit_transform(y)

dataset

print(X)

print(y)

# обучение на всем датасете
regressor = SVR(kernel = 'rbf')
regressor.fit(X, y)

regressor.score(X, y)

regressor.support_vectors_, regressor.intercept_, regressor.dual_coef_

# предсказание нового результата
sc_y.inverse_transform(regressor.predict(sc_X.transform([[6.5]])).reshape(-1,1))

dataset

# Визуализация SVR результатов
plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'red')
plt.plot(sc_X.inverse_transform(X), sc_y.inverse_transform(regressor.predict(X).reshape(-1,1)), color = 'blue')
plt.title('Truth or Bluff (SVR)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

# Визуализация результатов SVR (для получения более высокого разрешения и более плавной кривой)
X_grid = np.arange(min(sc_X.inverse_transform(X)), max(sc_X.inverse_transform(X)), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'red')
plt.plot(X_grid, sc_y.inverse_transform(regressor.predict(sc_X.transform(X_grid)).reshape(-1,1)), color = 'blue')
plt.title('Truth or Bluff (SVR)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

"""# Дерево решений Decision Tree

Про дерево решений хорошо написано в статье [хабра](https://habr.com/ru/companies/ods/articles/322534/) от сообщества ODS

![image.png](attachment:d0c2278a-99bf-4d75-a3fa-8507ac2e0480.png)
"""

path

dataset = pd.read_csv(path)
X = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values

regressor = DecisionTreeRegressor(random_state = 0)
regressor.fit(X, y)

regressor.score(X, y)

regressor.predict([[6.5]]) # новое предсказание

X_grid = np.arange(min(X), max(X), 0.01)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('Truth or Bluff (Decision Tree Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

"""# Random Forest

https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html
"""

dataset = pd.read_csv(path)
X = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values

regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)
regressor.fit(X, y)

dataset

regressor.predict([[6.5]]) # предсказание нового результата

regressor.score(X,y)

X_grid = np.arange(min(X), max(X), 0.01)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('Truth or Bluff (Random Forest Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

"""# ДЗ №2

В папке `practice_4` есть ноутбук EDA_AUTO. В этом ноутбуке проведен разведочный анализ данных, поиск и замена пропусков, визуализация данных, поиск выбросов/аномалий. Выполнить следующие задания:

1. Изучить ноутбук самостоятельно.
2. Столбец `price` - отклик, таргет. Остальные предикторы.
3. Столбец `make` не использовать для моделирования.  
4. Построить модель линейной регрессии и полиномиальной регрессии `price~horsepower`. Т.е. `price` - целевой отклик, `horsepower` - предиктор.
5. Построить модели множественной регрессии, SVM, Decision Tree, Random Forest, используя только числовые предикторы.
6. Построить модели множественной регрессии, SVM, Decision Tree, Random Forest, используя совместно с категориальными и числовыми предикторами.
7. Сравнить между собой модели. Рассчитать метрики и сделать визуализации
8. Использовать масштабирование признаков StandardScaler, и преобразование категориальных переменных

Для сокращения времени разработки рекомендую использовать  [Pipeline()](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) или [make_pipeline()](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline)

Руководства на [русском](https://scikit-learn.ru/6-1-pipelines-and-composite-estimators/) и [английском](https://scikit-learn.org/stable/modules/compose.html) языках

[Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html) - отбор признаков перед подачей в модель
"""

# Загрузка данных
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data"
columns = ["symboling", "normalized-losses", "make", "fuel-type", "aspiration", "num-of-doors",
           "body-style", "drive-wheels", "engine-location", "wheel-base", "length", "width",
           "height", "curb-weight", "engine-type", "num-of-cylinders", "engine-size",
           "fuel-system", "bore", "stroke", "compression-ratio", "horsepower", "peak-rpm",
           "city-mpg", "highway-mpg", "price"]
dataset = pd.read_csv(url, names=columns, na_values="?")
dataset = dataset.drop('make', axis=1)
dataset = dataset.dropna()
new_dataset = dataset[['price', 'horsepower']]
y = new_dataset['price'].values
X = new_dataset.iloc[:, 1:].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
lin_reg = LinearRegression() # простая линейная регрессия
lin_reg.fit(X, y)
plt.scatter(X, y, color = 'red')
plt.plot(X, lin_reg.predict(X), color = 'blue')
plt.title('price to horsepower (Linear Regression)')
plt.xlabel('price')
plt.ylabel('horsepower')
plt.show()
poly_reg = PolynomialFeatures() # полиномиальная регрессия
X_poly = poly_reg.fit_transform(X)
lin_reg_2 = LinearRegression()
lin_reg_2.fit(X_poly, y)
plt.scatter(X, y, color = 'red')
plt.plot(X, lin_reg_2.predict(poly_reg.fit_transform(X)), color = 'blue')
plt.title('price to horsepower (Polynomial Regression)')
plt.xlabel('price')
plt.ylabel('horsepower')
plt.show()

X = dataset[['engine-size', 'curb-weight', 'highway-mpg', 'city-mpg', 'symboling']]
y = dataset['price']
polynomial = PolynomialFeatures(degree=3)
X_polynomial = polynomial.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_polynomial, y, test_size = 0.2, random_state = 0)
regressor = LinearRegression()
regressor.fit(X_train, y_train)
print(regressor.score(X_train, y_train), regressor.score(X_test, y_test))
y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2)
tree = DecisionTreeRegressor(random_state=0)
tree.fit(X_train, y_train)
print(tree.score(X_train, y_train), tree.score(X_test, y_test))
forest = RandomForestRegressor(random_state=0, n_estimators=6)
forest.fit(X_train, y_train)
print(forest.score(X_train, y_train), forest.score(X_test, y_test))
svr = SVR()
scaler_x, scaler_y = StandardScaler(), StandardScaler()
X_train = scaler_x.fit_transform(X_train)
y_train = scaler_y.fit_transform(y_train.values.reshape(-1, 1))
svr.fit(X_train, y_train)
X_test = scaler_x.fit_transform(X_test)
y_test =scaler_y.fit_transform(y_test.values.reshape(-1,1))
print(svr.score(X_train, y_train), svr.score(X_test, y_test))

X = dataset[['fuel-system', 'wheel-base', 'engine-size', 'horsepower', 'engine-type']]
y = dataset['price']
ohe = OneHotEncoder()
ohe.fit(X)
polynomial = PolynomialFeatures(degree=3)
X_polynomial = polynomial.fit_transform(ohe.transform(X))
poly_reg = PolynomialFeatures(degree=3)
X_train, X_test, y_train, y_test = train_test_split(X_polynomial, y, test_size = 0.2, random_state = 0)
regressor = LinearRegression()
regressor.fit(X_train, y_train)
print(regressor.score(X_train, y_train), regressor.score(X_test, y_test))
y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2)
tree = DecisionTreeRegressor(random_state=0)
tree.fit(X_train, y_train)
print(tree.score(X_train, y_train), tree.score(X_test, y_test))
forest = RandomForestRegressor(random_state=0, n_estimators=6)
forest.fit(X_train, y_train)
print(forest.score(X_train, y_train), forest.score(X_test, y_test))
svr = SVR()
scaler_x, scaler_y = StandardScaler(with_mean=False), StandardScaler(with_mean=False)
X_train = scaler_x.fit_transform(X_train)
y_train = scaler_y.fit_transform(y_train.values.reshape(-1, 1))
svr.fit(X_train, y_train)
X_test = scaler_x.fit_transform(X_test)
y_test = scaler_y.fit_transform(y_test.values.reshape(-1,1))
print(svr.score(X_train, y_train), svr.score(X_test, y_test))